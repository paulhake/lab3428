{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8cc4329",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "About Retrieval Augmented Generation\n",
    "Retrieval Augmented Generation (RAG) is a versatile pattern that can unlock a number of use cases requiring factual recall of information, such as querying a knowledge base in natural language.\n",
    "\n",
    "In its simplest form, RAG requires these steps:\n",
    "\n",
    "Extract knowledge base passages from documents (once)\n",
    "Create vector embedding representations of each passage in the knowledge base\n",
    "Retreive question from end user and generate vector embedding for it.\n",
    "Retrieve relevant passage(s) from knowledge base (for every user query) using vector similarity search\n",
    "Generate a response by feeding retrieved passage into a large language model (for every user query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05161426",
   "metadata": {},
   "source": [
    "## Embeddings and Vector Databases\n",
    "The current state-of-the-art in RAG is to create dense vector representations of the knowledge base in order to calculate the semantic similarity to a given user query.\n",
    "\n",
    "We can generate dense vector representations using embedding models. In this notebook, we use SentenceTransformers all-MiniLM-L6-v2 to embed both the knowledge base passages and user queries. all-MiniLM-L6-v2 is a performant open-source model that is small enough to run locally.\n",
    "\n",
    "A vector database is optimized for dense vector indexing and retrieval. This notebook uses Chroma, a user-friendly open-source vector database, licensed under Apache 2.0, which offers good speed and performance with the all-MiniLM-L6-v2 embedding model.\n",
    "\n",
    "To generate the final response to a query based on the retrieved passage, we leverage an open-source model, Flan-UL2 (20B), and include a prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebcdade",
   "metadata": {},
   "source": [
    "### About the example dataset\n",
    "The dataset used in this cookbook is a subset of nq_open, an open-source question answering dataset based on contents from Wikipedia. The selected subset includes the gold standard passages to answer the queries in the dataset, which enables evaluating the retrieval quality.\n",
    "\n",
    "You can select one of the two dataset available:\n",
    "\n",
    "nq910 - an information retrieval (a.k.a. search) data set extracted from Google's Natural Questions dataset.\n",
    "LongNQ - an end-to-end retrieval and answer dataset extracted from the same NQ dataset, but focused more on abstractive, longer-form question answering. The answers were modified for fluency by IBM Research.\n",
    "These datasets are available in the data assets.\n",
    "\n",
    "Limitations\n",
    "Given that we are leveraging a locally-hosted embedding model, data ingestion and querying speeds can be slow.\n",
    "\n",
    "Cookbook Structure\n",
    "Set-up dependencies\n",
    "Index knowledge base\n",
    "Generate a retrieval-augmented response\n",
    "Evaluate RAG performance on your data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022066be",
   "metadata": {},
   "source": [
    "#### Disclaimer\n",
    "The IBM GenAI Python library used in this notebook is currently in Beta and will change in the future.\n",
    "\n",
    "##### 1.1 Install the required dependencies\n",
    "\n",
    "Note that `ibm-generative-ai` requires `python>=3.9`. Ensure these pre-requisites are met before using this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f30e33-7fd5-476d-beb8-5cd59f32dbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb==0.4.2\n",
    "!pip install ibm-watson-machine-learning==1.0.311\n",
    "!pip install ipywidgets==8.0.7\n",
    "!pip install jupyter==1.0.0\n",
    "!pip install langchain==0.0.236\n",
    "!pip install matplotlib==3.7.2\n",
    "!pip install numpy==1.24.2\n",
    "!pip install pandas==1.5.3\n",
    "!pip install plotly==5.15.0\n",
    "!pip install pypdf==3.12.2\n",
    "!pip install python-dotenv==1.0.0\n",
    "!pip install requests==2.31.0\n",
    "!pip install urllib3==1.26.11\n",
    "!pip install rouge==1.0.1\n",
    "!pip install scikit-learn==1.2.2\n",
    "!pip install sentence-transformers==2.2.2\n",
    "!pip install streamlit==1.24.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f46269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "from typing import Optional, Any, Iterable, List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except ImportError:\n",
    "    raise ImportError(\"Could not import sentence_transformers: Please install sentence-transformers package.\")\n",
    "    \n",
    "try:\n",
    "    import chromadb\n",
    "    from chromadb.api.types import EmbeddingFunction\n",
    "except ImportError:\n",
    "    raise ImportError(\"Could not import chromdb: Please install chromadb package.\")\n",
    "    \n",
    "from typing import Dict, Optional, List\n",
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfea2fa",
   "metadata": {},
   "source": [
    "#### 1.3. Load credentials for `ibm-watson-machine-learning`\n",
    "\n",
    "\n",
    "```\n",
    "API_KEY=<your-api_key>\n",
    "IBM_CLOUD_URL=<your-url>\n",
    "PROJECT_ID=<your-project_id>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5a9bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "IBM_CLOUD_API_KEY = getpass(\"Enter your IBM CLoud API Key: \")\n",
    "IBM_CLOUD_URL= 'https://us-south.ml.cloud.ibm.com'\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "creds = {\n",
    "    \"url\": IBM_CLOUD_URL,\n",
    "    \"apikey\": IBM_CLOUD_API_KEY\n",
    "}\n",
    "project_id = PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f14b93b",
   "metadata": {},
   "source": [
    "## 2. Index knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a5760c",
   "metadata": {},
   "source": [
    "### 2.1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d4d118",
   "metadata": {},
   "source": [
    "Select one of the two dataset available:\n",
    "1. *nq910* - an Information Retrieval (a.k.a. search) data set extracted from Google's Natural Questions dataset.\n",
    "2. *LongNQ* - an end-to-end retrieval and answer dataset extracted from the same NQ dataset, but focused more on abstractive question answering.\n",
    "\n",
    "These datasets are provided under the /data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5d904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['LongNQ', 'nq910']\n",
    "dataset = datasets[0]    # The current dataset to use\n",
    "data_root = \"data\"\n",
    "data_dir = os.path.join(data_root, dataset)\n",
    "max_docs = -1\n",
    "print(\"Selected dataset:\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "cos_client = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='g5DSKZVs5CucX2Rq4RJI-5vP9VYpcHpyl15bNbT90Nmo',\n",
    "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3.private.us-south.cloud-object-storage.appdomain.cloud')\n",
    "\n",
    "bucket = 'watsonxaifundamentals-donotdelete-pr-lsrsglkyrlpqqd'\n",
    "object_key = 'questions.csv'\n",
    "\n",
    "body = cos_client.get_object(Bucket=bucket,Key=object_key)['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "questions = pd.read_csv(body)\n",
    "questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41330644",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "cos_client = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='g5DSKZVs5CucX2Rq4RJI-5vP9VYpcHpyl15bNbT90Nmo',\n",
    "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3.private.us-south.cloud-object-storage.appdomain.cloud')\n",
    "\n",
    "bucket = 'watsonxaifundamentals-donotdelete-pr-lsrsglkyrlpqqd'\n",
    "object_key = 'output.csv'\n",
    "\n",
    "body = cos_client.get_object(Bucket=bucket,Key=object_key)['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "documents = pd.read_csv(body)\n",
    "documents.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2fe3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[\"id\"] = documents.rename({' id':'id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5d87ce",
   "metadata": {},
   "source": [
    "Selecting only 2000 examples to optimize time while creating embeddings. If we want to utilize the entire file for the pre-processing, comment the following and only use the entire dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9fde6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to read from local: Please uncomment these lines\n",
    "#passages = pd.read_csv(os.path.join(data_dir, \"passages.tsv\"), sep='\\t', header=0)\n",
    "#qas = pd.read_csv(os.path.join(data_dir, \"questions.tsv\"), sep='\\t', header=0).rename(columns={\"text\": \"question\"})\n",
    "\n",
    "# We only use 2000 examples.  Comment the lines below to use the full dataset.\n",
    "documents = documents.head(2000)\n",
    "questions = questions.head(2000)\n",
    "\n",
    "#documents, questions = load_data_v1(data_dir, data_root)\n",
    "documents['indextext'] = documents['title'].astype(str) + \"\\n\" + documents['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b06d45f",
   "metadata": {},
   "source": [
    "#### 1.2. Create an embedding function\n",
    "\n",
    "Note that you can feed a custom embedding function to be used by chromadb. The performance of chromadb may differ depending on the embedding model used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697431ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniLML6V2EmbeddingFunction(EmbeddingFunction):\n",
    "    MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    def __call__(self, texts):\n",
    "        return MiniLML6V2EmbeddingFunction.MODEL.encode(texts).tolist()\n",
    "emb_func = MiniLML6V2EmbeddingFunction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757aff54",
   "metadata": {},
   "source": [
    "#### 2.3. Set up Chroma upsert\n",
    "Upserting a document means update the document even if it exists in the database. Otherwise re-inserting a document throws an error. This is useful for experimentation purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be82bd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromaWithUpsert:\n",
    "    def __init__(self, name,persist_directory, embedding_function,collection_metadata: Optional[Dict] = None,\n",
    "    ):\n",
    "        self._client = chromadb.PersistentClient(path=persist_directory)\n",
    "        self._embedding_function = embedding_function\n",
    "        self._persist_directory = persist_directory\n",
    "        self._name = name\n",
    "        self._collection = self._client.get_or_create_collection(\n",
    "            name=self._name,\n",
    "            embedding_function=self._embedding_function\n",
    "            if self._embedding_function is not None\n",
    "            else None,\n",
    "            metadata=collection_metadata,\n",
    "        )\n",
    "\n",
    "    def upsert_texts(\n",
    "        self,\n",
    "        texts: Iterable[str],\n",
    "        metadata: Optional[List[dict]] = None,\n",
    "        ids: Optional[List[str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Run more texts through the embeddings and add to the vectorstore.\n",
    "        Args:\n",
    "            :param texts (Iterable[str]): Texts to add to the vectorstore.\n",
    "            :param metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n",
    "            :param ids (Optional[List[str]], optional): Optional list of IDs.\n",
    "            :param metadata: Optional[List[dict]] - optional metadata (such as title, etc.)\n",
    "        Returns:\n",
    "            List[str]: List of IDs of the added texts.\n",
    "        \"\"\"\n",
    "        # TODO: Handle the case where the user doesn't provide ids on the Collection\n",
    "        if ids is None:\n",
    "            import uuid\n",
    "            ids = [str(uuid.uuid1()) for _ in texts]\n",
    "        embeddings = None\n",
    "        self._collection.upsert(\n",
    "            metadatas=metadata, documents=texts, ids=ids\n",
    "        )\n",
    "        return ids\n",
    "\n",
    "    def is_empty(self):\n",
    "        return self._collection.count()==0\n",
    "\n",
    "    def query(self, query_texts:str, n_results:int=5):\n",
    "        \"\"\"\n",
    "        Returns the closests vector to the question vector\n",
    "        :param query_texts: the question\n",
    "        :param n_results: number of results to generate\n",
    "        :return: the closest result to the given question\n",
    "        \"\"\"\n",
    "        return self._collection.query(query_texts=query_texts, n_results=n_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a934ce63",
   "metadata": {},
   "source": [
    "#### 2.4 Embed and index documents with Chroma\n",
    "You will now generate embeddings for the passages. This will take\n",
    "\n",
    "However if you want to full experience, then delete these files and rebuild them yourself. Note that creating the embeddings and indexes can take a long time. E.g. on a 2021 Macbook Pro, it took 45 mins to generate these files for the LongNQ dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad9b3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chroma = ChromaWithUpsert(\n",
    "    name=f\"{dataset}_minilm6v2\",\n",
    "    embedding_function=emb_func,  # you can have something here using /embed endpoint\n",
    "    persist_directory=data_dir,\n",
    ")\n",
    "if chroma.is_empty():\n",
    "    _ = chroma.upsert_texts(\n",
    "        texts=documents.indextext.tolist(),\n",
    "        # we handle tokenization, embedding, and indexing automatically. You can skip that and add your own embeddings as well\n",
    "        metadata=[{'title': title, 'id': id}\n",
    "                  for (title,id) in\n",
    "                  zip(documents.title, documents[' id'])],  # filter on these!\n",
    "        ids=[str(i) for i in documents[' id']],  # unique for each doc\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b286d93a",
   "metadata": {},
   "source": [
    "### 3. Generate a retrieval-augmented response to a question\n",
    "3.1. Instantiate watsonx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd7eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        GenParams.DECODING_METHOD: \"greedy\",\n",
    "        GenParams.MIN_NEW_TOKENS: 1,\n",
    "        GenParams.MAX_NEW_TOKENS: 100,\n",
    "        GenParams.TEMPERATURE: 0,\n",
    "    }\n",
    "model = Model(model_id='google/flan-ul2', params=params, credentials=creds, project_id=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234e4058",
   "metadata": {},
   "source": [
    "#### 3.2. Select a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9c6323",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_index = 23\n",
    "question_text = questions.text[question_index].strip(\"?\") + \"?\"\n",
    "print(question_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7bd857",
   "metadata": {},
   "source": [
    "#### 3.3. Retrieve relevant context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040ac1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_chunks = chroma.query(\n",
    "    query_texts=[question_text],\n",
    "    n_results=5,\n",
    ")\n",
    "for i, chunk in enumerate(relevant_chunks['documents'][0]):\n",
    "    print(\"=========\")\n",
    "    print(\"Paragraph index : \", relevant_chunks['ids'][0][i])\n",
    "    print(\"Paragraph : \", chunk)\n",
    "    print(\"Distance : \", relevant_chunks['distances'][0][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188dc761",
   "metadata": {},
   "source": [
    "#### 3.4. Feed the context and the question to `watsonx` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf42f1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(context, question_text):\n",
    "    return (f\"{context}\\n\\nPlease answer a question using this \"\n",
    "          + f\"text. \"\n",
    "          + f\"If the question is unanswerable, say \\\"unanswerable\\\".\"\n",
    "          + f\"Question: {question_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3207cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\\n\\n\".join(relevant_chunks[\"documents\"][0])\n",
    "prompt = make_prompt(context, question_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cfea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_text(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500b3614",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Question = \", question_text)\n",
    "print(\"Answer = \", response)\n",
    "print(\"Expected Answer(s) (may not be appear with exact wording in the dataset) = \", questions.answers[question_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573c1220",
   "metadata": {},
   "source": [
    "### 4. Evaluate RAG performance on your data\n",
    "Evaluating the performance of your Generative AI system is critical to ensuring happy end users. However evaluation also requires having a test dataset. In this case, the top passages that shoudl be return for each question.\n",
    "\n",
    "Note that we want to evaluate the performance of both (1) the embedding function plus (2) how well the GenAI model summarizes the results.\n",
    "\n",
    "So our test set must contain:\n",
    "\n",
    "The indexes of the passage(s) that contain the answer - i.e. the goldstandard passages (if the question is answerable by the knowledge base)\n",
    "The question's gold standard answer (this can be short or long-form)\n",
    "\n",
    "\n",
    "4.1. Evaluate the retrieval quality\n",
    "Were the correct passages returned via the similarity search functionality\n",
    "\n",
    "There are many ways to compute retrieval quality, namely how the information contained in the documents that are relevant to the question being asked. We're focusing here on success at given number of returns (aka recall at given levels), which is to say, given a fixed number of documents returned (e.g., 1, 3, 5), is the question's answer contained in them. The scores increase with the recall level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49901af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(questions, answers, ranks=[1, 3, 5, 10], use_rouge=False, rouge_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Computes the success at different levels of recall, given the goldstandard passage indexes per query.\n",
    "    It computes two scores:\n",
    "       * Success at rank_i, defined as sum_q 1_{top i answers for question q contains a goldstandard passage} / #questions\n",
    "       * Lenient success at rank i, defined as\n",
    "                sum_q 1_{ one in the documents in top i for question q contains a goldstandard answer) / #questions\n",
    "    Note that a document that contains the actual textual answer does not necesarily answer the question, hence it's a\n",
    "    more lenient evaluation. Any goldstandard passage will contain a goldstandard answer text, by definition.\n",
    "    Args:\n",
    "        :param questions: List[Dict['id': AnyStr, 'text': AnyStr, 'relevant': AnyStr, 'answers': AnyStr]]\n",
    "           - the input queries. Each query is a dictionary with the keys 'id','text', 'relevant', 'answers'.\n",
    "        :param input_passages: List[Dict['id': AnyStr, 'text': AnyStr', 'title': AnyStr]]\n",
    "           - the input passages. These are used to create a reverse-index list for the passages (so we can get the\n",
    "             text for a given passage ID)\n",
    "        :param answers: List[List[AnyStr]]\n",
    "           - the retrieved passages IDs for each query\n",
    "        :param ranks: List[int]\n",
    "           - the ranks at which to compute success\n",
    "        :param use_rouge: Boolean\n",
    "           - turns on the use of rouge as a scorer\n",
    "        :param rouge_threshold: float, default=0.7\n",
    "           - defines the minimum rouge-l/r score to accept the answer as a match,\n",
    "    Returns:\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # if \"relevant\" not in input_queries[0] or input_queries[0]['relevant'] is None:\n",
    "    #     print(\"The input question file does not contain answers. Please fix that and restart.\")\n",
    "    #     sys.exit(12)\n",
    "\n",
    "    scores = {r: 0 for r in ranks}\n",
    "    lscores = {r: 0 for r in ranks}\n",
    "\n",
    "    gt = {}\n",
    "    for q_relevant, q_qid in zip(questions.relevant, questions.qid):\n",
    "        if isinstance(q_relevant, str):\n",
    "            rel = [int(i) for i in q_relevant.split(\",\")]\n",
    "        else:\n",
    "            rel = [q_relevant]\n",
    "        gt[q_qid] = rel\n",
    "\n",
    "    def update_scores(ranks, rnk, scores):\n",
    "        j = 0\n",
    "        while j < len(ranks) and ranks[j] < rnk:\n",
    "            j += 1\n",
    "        for k in ranks[j:]:\n",
    "            scores[k] += 1\n",
    "\n",
    "    scorer = None\n",
    "    if use_rouge:\n",
    "        from rouge import Rouge\n",
    "        scorer = Rouge()\n",
    "\n",
    "    num_eval_questions = 0\n",
    "\n",
    "    for qi, (qid, q_answers) in enumerate(zip(questions.qid, questions.answers)):\n",
    "        tmp_scores = {r: 0 for r in ranks}\n",
    "\n",
    "        text_answers = str(q_answers).split(\"::\")\n",
    "        if \"-\" in text_answers:\n",
    "            # The question does not have answers, skip it for retrieval score purposes.\n",
    "            continue\n",
    "        num_eval_questions += 1\n",
    "        # Compute scores based on the goldstandard annotation\n",
    "        for ai, ans in enumerate(answers[qi]):\n",
    "            if int(ans['id']) in gt[qid]:  # Great, we found a match.\n",
    "                update_scores(ranks, ai + 1, tmp_scores)\n",
    "                break\n",
    "\n",
    "        # Compute score on approximate match - either answer inclusion in the text or\n",
    "        # minimum rouge score alignment.\n",
    "        tmp_lscores = tmp_scores.copy()  # making sure we're actually lenient\n",
    "        #inputq = questions[qi]\n",
    "        for ai, ans in enumerate(answers[qi]):\n",
    "            txt = ans['text'].lower()\n",
    "            found = False\n",
    "            for text_answer in text_answers:\n",
    "                if use_rouge:\n",
    "                    score = scorer.get_scores(text_answer.lower(), txt)\n",
    "                    if max(score[0]['rouge-l']['r'], score[0]['rouge-l']['p']) > rouge_threshold:\n",
    "                        update_scores(ranks, ai + 1, tmp_lscores)\n",
    "                        break\n",
    "                else:\n",
    "                    if not isinstance(text_answer, str):\n",
    "                        print(f\"Error on text_answer {text_answer}, question {qi}, answer {ai}-{ans}\")\n",
    "                    if txt.find(text_answer.lower()) >= 1:\n",
    "                        update_scores(ranks, ai + 1, tmp_lscores)\n",
    "                        break\n",
    "\n",
    "        for r in ranks:\n",
    "            scores[r] += int(tmp_scores[r] >= 1)\n",
    "            lscores[r] += int(tmp_lscores[r] >= 1)\n",
    "\n",
    "    res = {\"num_ranked_queries\": num_eval_questions,\n",
    "           \"num_judged_queries\": num_eval_questions,\n",
    "           \"success\":\n",
    "               {r: int(1000 * scores[r] / num_eval_questions) / 1000.0 for r in ranks},\n",
    "           \"lenient_success\":\n",
    "               {r: int(1000 * lscores[r] / num_eval_questions) / 1000.0 for r in ranks},\n",
    "           \"counts\": {r: scores[r] for r in ranks},\n",
    "           'lcounts': {r: lscores[r] for r in ranks}\n",
    "           }\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7180371",
   "metadata": {},
   "source": [
    "#### Compute the retrieval score over all the documents\n",
    "Can take up to a minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c5b9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "retrieved_docs = []\n",
    "for q in questions.text:\n",
    "    answers = chroma.query(query_texts=q, n_results=k)\n",
    "\n",
    "    retrieved_docs.append([{'id': id, 'text': text}\n",
    "                           for (id, text) in zip(answers['ids'][0], answers['documents'][0])])\n",
    "\n",
    "res = compute_score(questions, retrieved_docs,\n",
    "                    ranks=[1, 3, 5], use_rouge=(data_dir == 'docs_and_qs'))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0950159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def plot(res):\n",
    "    fig, ax = plt.subplots()\n",
    "    scores = res['success'].values()\n",
    "    keys = [f'R@{i}' for i in res['success'].keys()]\n",
    "    x_pos = np.arange(len(keys))\n",
    "    ax.bar(x_pos, scores, align='center', alpha=0.5)\n",
    "    ax.set_ylabel('Success Rate')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(keys)\n",
    "    ax.set_title('Success rates at different recall rates.')\n",
    "    ax.yaxis.grid(True)\n",
    "\n",
    "    # Save the figure and show\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('bar_plot.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f4e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dfafb1",
   "metadata": {},
   "source": [
    "### 4.2. Evaluate quality of generated responses\n",
    "I.e. how well did the GenAI model summarize and extract the correct answer to the user's question from the passages returned by the similarity function.  Obviously if the returned passages were invalid, then performance at this phase would suffer too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e72e5f",
   "metadata": {},
   "source": [
    "##### Automatically evaluating the quality of answers is difficult, as many factors come into play, such as fluency, helpfulness, coverage, etc. One simplified way of computing this quality is using the ROUGE metric, in particular ROUGE-L. To compute this metric, for every answer returned for a question, we measure the maximum subsequence of words between the system answer and the gold-standard answer. Given this sequence, we can compute the precision of the given answer as the length (all lengths are in words) of this sequence divided by the length of the system answer and the recall as the length of the longest common subsequence divided by the length the gold-standard answer.\n",
    "$$ P_{ROUGE-L} = \\frac{|lcs(system,gold)|}{|system|} \\\\ R_{ROUGE_L} = \\frac{|lcs(system,gold|}{|gold|} $$\n",
    "\n",
    "where $lcs(system, gold)$ is the longest commong subsequence between $system$ and $gold$.\n",
    "\n",
    "ROUGE was devised in the NLP community to evaluate summarization, and is commonly used to also evaluate abstractive question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23b22b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_answers(_answers, _reference, score_type=\"rouge-l\", val=\"r\", use_rouge=True):\n",
    "    \"\"\"\n",
    "    Compute the score of a set of answers, given a set of references, using Rouge score.\n",
    "    :param answers: Union[List[str], str]\n",
    "       - the returned answer/answers.\n",
    "    :param reference:\n",
    "        - the reference answers, in a string. Answers are separated by ':::'\n",
    "    :param use_rouge: Boolean\n",
    "        - if true, then use rouge for scoring, otherwise use substring.\n",
    "    :return:\n",
    "       - The maximum rouge-L score of the cartesian product of answers/references\n",
    "    \"\"\"\n",
    "    if isinstance(_answers, str):\n",
    "        _answers = [_answers]\n",
    "    _references = _reference.lower().split(\"::\")\n",
    "    max_score = -1\n",
    "    scorer = Rouge()\n",
    "    closest_ref = \"\"\n",
    "    for ref in _references:\n",
    "        for _answer in _answers:\n",
    "            if use_rouge:\n",
    "                scores = scorer.get_scores(_answer.lower(), ref)\n",
    "                score = scores[0][score_type][val]\n",
    "            else:\n",
    "                score = int(ref.find(_answer.lower()) >= 0)\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                closest_ref = ref\n",
    "\n",
    "    return max_score, closest_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b48cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Question = \", question_text)\n",
    "print(\"Answer = \", response)\n",
    "score, closest_ref = score_answers(response, questions.answers[question_index], val='r')\n",
    "print(f\"Closest reference: \\\"{closest_ref}\\\"\")\n",
    "print(f\"Recall:\\t\\t{100*score:5.2f}%\")\n",
    "score, _ = score_answers(response, questions.answers[question_index], val='p')\n",
    "print(f\"Precision:\\t{100*score:5.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f04a79",
   "metadata": {},
   "source": [
    "#### Compute (Rouge-based) precision and recall for the entire collection.\n",
    "It takes about 1-2 seconds per question. For a corpus of ~1000 questions, this take can take up to 30min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ca204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_answerable(relevant):\n",
    "    return \"-1\" in relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5db342",
   "metadata": {},
   "outputs": [],
   "source": [
    "rscore = 0\n",
    "pscore = 0\n",
    "import tqdm\n",
    "num_eval_questions = 50\n",
    "eval_questions = questions[:num_eval_questions]\n",
    "count = {\"11\": 0, \"10\": 0, \"01\": 0, \"00\": 0}\n",
    "seq = []\n",
    "for (question_text, answers, relevant) in tqdm.tqdm(zip(eval_questions.text, eval_questions.answers, eval_questions.relevant), total=len(eval_questions)):\n",
    "    # ans = qa(question.question)\n",
    "    relevant_chunks = chroma.query(\n",
    "        query_texts=[question_text],\n",
    "        n_results=5,\n",
    "    )\n",
    "    context = \"\\n\\n\\n\".join(relevant_chunks[\"documents\"][0])\n",
    "    prompt = make_prompt(context, question_text)\n",
    "    ans = model.generate_text(prompt)\n",
    "    q_answerable = is_answerable(relevant)\n",
    "    if ans == \"unanswerable\":\n",
    "        res = \"10\" if q_answerable else \"00\"\n",
    "        count [res] += 1\n",
    "        if not q_answerable:\n",
    "            rscore += 1\n",
    "            pscore += 1\n",
    "    else:\n",
    "        res = \"11\" if q_answerable else \"10\"\n",
    "        count[res] += 1\n",
    "        if q_answerable:\n",
    "            qrscore, _ = score_answers(ans, answers, val='r')\n",
    "            rscore += qrscore\n",
    "            qpscore, _ = score_answers(ans, answers, val='p')\n",
    "            pscore += qpscore\n",
    "    seq.append(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b12b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "def displayHTMLTables(*tables):\n",
    "    def htmlTable(table):\n",
    "        return '<table border=\"2\"><tr>{}</tr></table>'.format(\n",
    "                    '</tr><tr>'.join(\n",
    "                        '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in table)\n",
    "                )\n",
    "\n",
    "    display(HTML('<table><tr><td>{}</td></tr></table>'.format(\n",
    "                \"</td><td>\".join(htmlTable(table) for table in tables))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f85bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [['', 'Overall', 'Answerable questions'],\n",
    "       ['Precision', f\"{100*pscore/len(eval_questions):5.2f}\", f\"{100*(pscore-count['00'])/(count['10']+count['11']):5.2f}\"],\n",
    "       ['Recall',    f'{100*pscore/len(eval_questions):5.2f}', f\"{188*(rscore-count['00'])/(count['10']+count['11']):5.2f}\"],\n",
    "       ]\n",
    "counts = [['Gold/System', 'No Answer', 'Answered'],\n",
    "        ['No Answer', count[\"00\"], count[\"01\"]],\n",
    "        ['With Answer', count[\"10\"], count[\"11\"]]]#%% md\n",
    "\n",
    "displayHTMLTables(res, [], [], counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

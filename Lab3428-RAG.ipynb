{"cells": [{"metadata": {}, "id": "a8cc4329", "cell_type": "markdown", "source": "# Introduction\nAbout Retrieval Augmented Generation\nRetrieval Augmented Generation (RAG) is a versatile pattern that can unlock a number of use cases requiring factual recall of information, such as querying a knowledge base in natural language.\n\nIn its simplest form, RAG requires these steps:\n\nExtract knowledge base passages from documents (once)\nCreate vector embedding representations of each passage in the knowledge base\nRetreive question from end user and generate vector embedding for it.\nRetrieve relevant passage(s) from knowledge base (for every user query) using vector similarity search\nGenerate a response by feeding retrieved passage into a large language model (for every user query)"}, {"metadata": {}, "id": "05161426", "cell_type": "markdown", "source": "## Embeddings and Vector Databases\nThe current state-of-the-art in RAG is to create dense vector representations of the knowledge base in order to calculate the semantic similarity to a given user query.\n\nWe can generate dense vector representations using embedding models. In this notebook, we use SentenceTransformers all-MiniLM-L6-v2 to embed both the knowledge base passages and user queries. all-MiniLM-L6-v2 is a performant open-source model that is small enough to run locally.\n\nA vector database is optimized for dense vector indexing and retrieval. This notebook uses Chroma, a user-friendly open-source vector database, licensed under Apache 2.0, which offers good speed and performance with the all-MiniLM-L6-v2 embedding model.\n\nTo generate the final response to a query based on the retrieved passage, we leverage an open-source model, Flan-UL2 (20B), and include a prompt"}, {"metadata": {}, "id": "4ebcdade", "cell_type": "markdown", "source": "### About the example dataset\nThe dataset used in this cookbook is a subset of nq_open, an open-source question answering dataset based on contents from Wikipedia. The selected subset includes the gold standard passages to answer the queries in the dataset, which enables evaluating the retrieval quality.\n\nYou can select one of the two dataset available:\n\nnq910 - an information retrieval (a.k.a. search) data set extracted from Google's Natural Questions dataset.\nLongNQ - an end-to-end retrieval and answer dataset extracted from the same NQ dataset, but focused more on abstractive, longer-form question answering. The answers were modified for fluency by IBM Research.\nThese datasets are available in the data assets.\n\nLimitations\nGiven that we are leveraging a locally-hosted embedding model, data ingestion and querying speeds can be slow.\n\nCookbook Structure\nSet-up dependencies\nIndex knowledge base\nGenerate a retrieval-augmented response\nEvaluate RAG performance on your data\n"}, {"metadata": {}, "id": "022066be", "cell_type": "markdown", "source": "#### Disclaimer\nThe IBM GenAI Python library used in this notebook is currently in Beta and will change in the future.\n\n##### 1.1 Install the required dependencies\n\nNote that `ibm-generative-ai` requires `python>=3.9`. Ensure these pre-requisites are met before using this notebook"}, {"metadata": {}, "id": "43f30e33-7fd5-476d-beb8-5cd59f32dbdf", "cell_type": "code", "source": "!pip install chromadb==0.4.2\n!pip install ibm-watson-machine-learning==1.0.311\n!pip install ipywidgets==8.0.7\n!pip install jupyter==1.0.0\n!pip install langchain==0.0.236\n!pip install matplotlib==3.7.2\n!pip install numpy==1.24.2\n!pip install pandas==1.5.3\n!pip install plotly==5.15.0\n!pip install pypdf==3.12.2\n!pip install python-dotenv==1.0.0\n!pip install requests==2.31.0\n!pip install urllib3==1.26.11\n!pip install rouge==1.0.1\n!pip install scikit-learn==1.2.2\n!pip install sentence-transformers==2.2.2\n!pip install streamlit==1.24.1", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Collecting chromadb==0.4.2\n  Downloading chromadb-0.4.2-py3-none-any.whl (399 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m399.3/399.3 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas>=1.3 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from chromadb==0.4.2) (1.4.3)\nRequirement already satisfied: requests>=2.28 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from chromadb==0.4.2) (2.31.0)\nCollecting onnxruntime>=1.14.1\n  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting typing-extensions>=4.5.0\n  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\nCollecting overrides>=7.3.1\n  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\nCollecting pypika>=0.48.9\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting chroma-hnswlib==0.7.1\n  Downloading chroma-hnswlib-0.7.1.tar.gz (30 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting tokenizers>=0.13.2\n  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting fastapi<0.100.0,>=0.95.2\n  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pulsar-client>=3.1.0\n  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.21.6 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from chromadb==0.4.2) (1.23.1)\nCollecting posthog>=2.4.0\n  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\nCollecting tqdm>=4.65.0\n  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting uvicorn[standard]>=0.18.3\n  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pydantic<2.0,>=1.9\n  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting importlib-resources\n  Downloading importlib_resources-6.0.1-py3-none-any.whl (34 kB)\nCollecting starlette<0.28.0,>=0.27.0\n  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: protobuf in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.2) (3.19.6)\nRequirement already satisfied: flatbuffers in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.2) (2.0)\nCollecting coloredlogs\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sympy\n  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.2) (21.3)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pandas>=1.3->chromadb==0.4.2) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pandas>=1.3->chromadb==0.4.2) (2022.1)\nCollecting backoff>=1.10.0\n  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.2) (1.16.0)\nCollecting monotonic>=1.5\n  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nRequirement already satisfied: certifi in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pulsar-client>=3.1.0->chromadb==0.4.2) (2023.7.22)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.4.2) (3.3)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.4.2) (1.26.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.4.2) (2.0.4)\nCollecting huggingface_hub<0.17,>=0.16.4\n  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: click>=7.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.2) (8.0.4)\nCollecting h11>=0.8\n  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting python-dotenv>=0.13\n  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\nCollecting websockets>=10.4\n  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting watchfiles>=0.13\n  Downloading watchfiles-0.20.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting httptools>=0.5.0\n  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.2) (6.0)\nCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0\n  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting filelock\n  Downloading filelock-3.12.3-py3-none-any.whl (11 kB)\nRequirement already satisfied: fsspec in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.2) (2021.10.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from packaging->onnxruntime>=1.14.1->chromadb==0.4.2) (3.0.9)\nCollecting anyio<5,>=3.4.0\n  Downloading anyio-4.0.0-py3-none-any.whl (83 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m83.1/83.1 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "name": "stdout"}, {"output_type": "stream", "text": "\u001b[?25hCollecting humanfriendly>=9.1\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting mpmath>=0.19\n  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting exceptiongroup>=1.0.2\n  Downloading exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\nCollecting sniffio>=1.1\n  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\nBuilding wheels for collected packages: chroma-hnswlib, pypika\n  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.1-cp310-cp310-linux_x86_64.whl size=181725 sha256=e136c5fcd60899732ac0ba5d4f89ec442105c8b147c25c8fade2d369dada546b\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/ad/f2/d2/3f32228e9f4713a9f32a468de8bbc3c642f7805ebef888418b\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53738 sha256=50b56fe5a0a58967e9d85ba50ee211f754cf801e17891e3157dfed43d854e6b8\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built chroma-hnswlib pypika\nInstalling collected packages: pypika, mpmath, monotonic, websockets, uvloop, typing-extensions, tqdm, sympy, sniffio, python-dotenv, pulsar-client, overrides, importlib-resources, humanfriendly, httptools, h11, exceptiongroup, chroma-hnswlib, backoff, uvicorn, pydantic, posthog, filelock, coloredlogs, anyio, watchfiles, starlette, onnxruntime, huggingface_hub, tokenizers, fastapi, chromadb\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.3.0\n    Uninstalling typing_extensions-4.3.0:\n      Successfully uninstalled typing_extensions-4.3.0\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.64.0\n    Uninstalling tqdm-4.64.0:\n      Successfully uninstalled tqdm-4.64.0\nSuccessfully installed anyio-4.0.0 backoff-2.2.1 chroma-hnswlib-0.7.1 chromadb-0.4.2 coloredlogs-15.0.1 exceptiongroup-1.1.3 fastapi-0.99.1 filelock-3.12.3 h11-0.14.0 httptools-0.6.0 huggingface_hub-0.16.4 humanfriendly-10.0 importlib-resources-6.0.1 monotonic-1.6 mpmath-1.3.0 onnxruntime-1.15.1 overrides-7.4.0 posthog-3.0.2 pulsar-client-3.3.0 pydantic-1.10.12 pypika-0.48.9 python-dotenv-1.0.0 sniffio-1.3.0 starlette-0.27.0 sympy-1.12 tokenizers-0.14.0 tqdm-4.66.1 typing-extensions-4.7.1 uvicorn-0.23.2 uvloop-0.17.0 watchfiles-0.20.0 websockets-11.0.3\nCollecting ibm-watson-machine-learning==1.0.311\n  Downloading ibm_watson_machine_learning-1.0.311-py3-none-any.whl (1.7 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning==1.0.311) (4.11.3)\nRequirement already satisfied: requests in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning==1.0.311) (2.31.0)\nRequirement already satisfied: ibm-cos-sdk<2.14.0,>=2.12.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning==1.0.311) (2.12.0)\nRequirement already satisfied: urllib3 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning==1.0.311) (1.26.11)\nRequirement already satisfied: lomond in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning==1.0.311) (0.3.3)\nRequirement already satisfied: tabulate in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning==1.0.311) (0.8.10)\nRequirement already satisfied: certifi in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning==1.0.311) (2023.7.22)\nRequirement already satisfied: pandas<1.6.0,>=0.24.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning==1.0.311) (1.4.3)\nRequirement already satisfied: packaging in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-watson-machine-learning==1.0.311) (21.3)\nRequirement already satisfied: ibm-cos-sdk-s3transfer==2.12.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning==1.0.311) (2.12.0)\nRequirement already satisfied: jmespath<1.0.0,>=0.10.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning==1.0.311) (0.10.0)\nRequirement already satisfied: ibm-cos-sdk-core==2.12.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning==1.0.311) (2.12.0)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ibm-cos-sdk-core==2.12.0->ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning==1.0.311) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pandas<1.6.0,>=0.24.2->ibm-watson-machine-learning==1.0.311) (2022.1)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pandas<1.6.0,>=0.24.2->ibm-watson-machine-learning==1.0.311) (1.23.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->ibm-watson-machine-learning==1.0.311) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->ibm-watson-machine-learning==1.0.311) (3.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from importlib-metadata->ibm-watson-machine-learning==1.0.311) (3.8.0)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from lomond->ibm-watson-machine-learning==1.0.311) (1.16.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from packaging->ibm-watson-machine-learning==1.0.311) (3.0.9)\nInstalling collected packages: ibm-watson-machine-learning\n  Attempting uninstall: ibm-watson-machine-learning\n    Found existing installation: ibm-watson-machine-learning 1.0.320\n    Uninstalling ibm-watson-machine-learning-1.0.320:\n      Successfully uninstalled ibm-watson-machine-learning-1.0.320\nSuccessfully installed ibm-watson-machine-learning-1.0.311\nCollecting ipywidgets==8.0.7\n  Downloading ipywidgets-8.0.7-py3-none-any.whl (138 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: traitlets>=4.3.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ipywidgets==8.0.7) (5.1.1)\nCollecting jupyterlab-widgets~=3.0.7\n  Downloading jupyterlab_widgets-3.0.8-py3-none-any.whl (214 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m215.0/215.0 kB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: ipython>=6.1.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ipywidgets==8.0.7) (8.4.0)\nCollecting widgetsnbextension~=4.0.7\n  Downloading widgetsnbextension-4.0.8-py3-none-any.whl (2.3 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: ipykernel>=4.5.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ipywidgets==8.0.7) (6.9.1)\nRequirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets==8.0.7) (1.5.1)\nRequirement already satisfied: jupyter-client<8.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets==8.0.7) (7.3.5)\nRequirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets==8.0.7) (6.3.2)\nRequirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets==8.0.7) (0.1.6)\nRequirement already satisfied: nest-asyncio in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets==8.0.7) (1.5.5)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.0.7) (0.18.1)\nRequirement already satisfied: setuptools>=18.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.0.7) (65.6.3)\nRequirement already satisfied: decorator in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.0.7) (5.1.1)\nRequirement already satisfied: backcall in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.0.7) (0.2.0)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.0.7) (2.11.2)\nRequirement already satisfied: stack-data in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.0.7) (0.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.0.7) (4.8.0)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.0.7) (3.0.20)\nRequirement already satisfied: pickleshare in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets==8.0.7) (0.7.5)\n", "name": "stdout"}, {"output_type": "stream", "text": "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets==8.0.7) (0.8.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets==8.0.7) (2.8.2)\nRequirement already satisfied: entrypoints in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets==8.0.7) (0.4)\nRequirement already satisfied: jupyter-core>=4.9.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets==8.0.7) (4.11.2)\nRequirement already satisfied: pyzmq>=23.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets==8.0.7) (23.2.0)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets==8.0.7) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets==8.0.7) (0.2.5)\nRequirement already satisfied: asttokens in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.0.7) (2.0.5)\nRequirement already satisfied: pure-eval in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.0.7) (0.2.2)\nRequirement already satisfied: executing in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.0.7) (0.8.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets==8.0.7) (1.16.0)\nInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n  Attempting uninstall: widgetsnbextension\n    Found existing installation: widgetsnbextension 3.5.2\n    Uninstalling widgetsnbextension-3.5.2:\n      Successfully uninstalled widgetsnbextension-3.5.2\n  Attempting uninstall: jupyterlab-widgets\n    Found existing installation: jupyterlab-widgets 1.0.0\n    Uninstalling jupyterlab-widgets-1.0.0:\n      Successfully uninstalled jupyterlab-widgets-1.0.0\n  Attempting uninstall: ipywidgets\n    Found existing installation: ipywidgets 7.6.5\n    Uninstalling ipywidgets-7.6.5:\n      Successfully uninstalled ipywidgets-7.6.5\nSuccessfully installed ipywidgets-8.0.7 jupyterlab-widgets-3.0.8 widgetsnbextension-4.0.8\nRequirement already satisfied: jupyter==1.0.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (1)\nCollecting langchain==0.0.236\n  Downloading langchain-0.0.236-py3-none-any.whl (1.3 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting openapi-schema-pydantic<2.0,>=1.2\n  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from langchain==0.0.236) (6.0)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from langchain==0.0.236) (1.4.39)\nCollecting aiohttp<4.0.0,>=3.8.3\n  Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting numexpr<3.0.0,>=2.8.4\n  Downloading numexpr-2.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (383 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m383.2/383.2 kB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from langchain==0.0.236) (1.23.1)\nCollecting dataclasses-json<0.6.0,>=0.5.7\n  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from langchain==0.0.236) (4.0.1)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from langchain==0.0.236) (2.31.0)\nCollecting langsmith<0.0.11,>=0.0.10\n  Downloading langsmith-0.0.10-py3-none-any.whl (27 kB)\nRequirement already satisfied: pydantic<2,>=1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from langchain==0.0.236) (1.10.12)\nCollecting tenacity<9.0.0,>=8.1.0\n  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.236) (2.0.4)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.236) (1.2.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.236) (1.8.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.236) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.236) (21.4.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.236) (5.2.0)\nRequirement already satisfied: typing-extensions>=3.6.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from async-timeout<5.0.0,>=4.0.0->langchain==0.0.236) (4.7.1)\nCollecting typing-inspect<1,>=0.4.0\n  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nCollecting marshmallow<4.0.0,>=3.18.0\n  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.236) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.236) (2023.7.22)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.0.236) (1.26.11)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.236) (1.1.1)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.236) (21.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.236) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.236) (3.0.9)\nInstalling collected packages: typing-inspect, tenacity, numexpr, openapi-schema-pydantic, marshmallow, langsmith, aiohttp, dataclasses-json, langchain\n  Attempting uninstall: tenacity\n    Found existing installation: tenacity 8.0.1\n    Uninstalling tenacity-8.0.1:\n      Successfully uninstalled tenacity-8.0.1\n  Attempting uninstall: numexpr\n    Found existing installation: numexpr 2.8.3\n    Uninstalling numexpr-2.8.3:\n      Successfully uninstalled numexpr-2.8.3\n  Attempting uninstall: aiohttp\n    Found existing installation: aiohttp 3.8.1\n    Uninstalling aiohttp-3.8.1:\n      Successfully uninstalled aiohttp-3.8.1\n", "name": "stdout"}, {"output_type": "stream", "text": "Successfully installed aiohttp-3.8.5 dataclasses-json-0.5.14 langchain-0.0.236 langsmith-0.0.10 marshmallow-3.20.1 numexpr-2.8.5 openapi-schema-pydantic-1.2.4 tenacity-8.2.3 typing-inspect-0.9.0\nCollecting matplotlib==3.7.2\n  Downloading matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.20 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from matplotlib==3.7.2) (1.23.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from matplotlib==3.7.2) (4.25.0)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from matplotlib==3.7.2) (9.3.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from matplotlib==3.7.2) (2.8.2)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from matplotlib==3.7.2) (0.11.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from matplotlib==3.7.2) (21.3)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from matplotlib==3.7.2) (3.0.9)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from matplotlib==3.7.2) (1.4.2)\nCollecting contourpy>=1.0.1\n  Downloading contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m300.7/300.7 kB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib==3.7.2) (1.16.0)\nInstalling collected packages: contourpy, matplotlib\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.5.2\n    Uninstalling matplotlib-3.5.2:\n      Successfully uninstalled matplotlib-3.5.2\nSuccessfully installed contourpy-1.1.0 matplotlib-3.7.2\nCollecting numpy==1.24.2\n  Downloading numpy-1.24.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.23.1\n    Uninstalling numpy-1.23.1:\n      Successfully uninstalled numpy-1.23.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nnumba 0.56.1 requires numpy<1.24,>=1.18, but you have numpy 1.24.2 which is incompatible.\nautoai-ts-libs 2.0.13 requires numpy<1.24,>=1.19.2; python_version >= \"3.9\", but you have numpy 1.24.2 which is incompatible.\nautoai-libs 1.14.10 requires numpy<1.24,>=1.19.2; python_version >= \"3.9\", but you have numpy 1.24.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.24.2\nCollecting pandas==1.5.3\n  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pandas==1.5.3) (1.24.2)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pandas==1.5.3) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pandas==1.5.3) (2022.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\nInstalling collected packages: pandas\n  Attempting uninstall: pandas\n    Found existing installation: pandas 1.4.3\n    Uninstalling pandas-1.4.3:\n      Successfully uninstalled pandas-1.4.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nautoai-ts-libs 2.0.13 requires numpy<1.24,>=1.19.2; python_version >= \"3.9\", but you have numpy 1.24.2 which is incompatible.\nautoai-libs 1.14.10 requires numpy<1.24,>=1.19.2; python_version >= \"3.9\", but you have numpy 1.24.2 which is incompatible.\nautoai-libs 1.14.10 requires pandas<1.5,>=0.24.2, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pandas-1.5.3\nCollecting plotly==5.15.0\n  Downloading plotly-5.15.0-py2.py3-none-any.whl (15.5 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from plotly==5.15.0) (8.2.3)\nRequirement already satisfied: packaging in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from plotly==5.15.0) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from packaging->plotly==5.15.0) (3.0.9)\nInstalling collected packages: plotly\n  Attempting uninstall: plotly\n    Found existing installation: plotly 5.9.0\n    Uninstalling plotly-5.9.0:\n      Successfully uninstalled plotly-5.9.0\nSuccessfully installed plotly-5.15.0\nCollecting pypdf==3.12.2\n  Downloading pypdf-3.12.2-py3-none-any.whl (254 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m255.0/255.0 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pypdf\nSuccessfully installed pypdf-3.12.2\nRequirement already satisfied: python-dotenv==1.0.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (1.0.0)\nRequirement already satisfied: requests==2.31.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (2.31.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests==2.31.0) (2023.7.22)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests==2.31.0) (3.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests==2.31.0) (2.0.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests==2.31.0) (1.26.11)\nRequirement already satisfied: urllib3==1.26.11 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (1.26.11)\nCollecting rouge==1.0.1\n  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\nRequirement already satisfied: six in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from rouge==1.0.1) (1.16.0)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\nCollecting scikit-learn==1.2.2\n  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from scikit-learn==1.2.2) (1.24.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from scikit-learn==1.2.2) (1.1.1)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from scikit-learn==1.2.2) (1.8.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from scikit-learn==1.2.2) (2.2.0)\nInstalling collected packages: scikit-learn\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.1.1\n", "name": "stdout"}, {"output_type": "stream", "text": "    Uninstalling scikit-learn-1.1.1:\n      Successfully uninstalled scikit-learn-1.1.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlale 0.7.1 requires scikit-learn<=1.1.1,>=1.0.0, but you have scikit-learn 1.2.2 which is incompatible.\nautoai-ts-libs 2.0.13 requires numpy<1.24,>=1.19.2; python_version >= \"3.9\", but you have numpy 1.24.2 which is incompatible.\nautoai-ts-libs 2.0.13 requires scikit-learn<=1.1.1,>=1.0.2; python_version >= \"3.9\", but you have scikit-learn 1.2.2 which is incompatible.\nautoai-libs 1.14.10 requires numpy<1.24,>=1.19.2; python_version >= \"3.9\", but you have numpy 1.24.2 which is incompatible.\nautoai-libs 1.14.10 requires pandas<1.5,>=0.24.2, but you have pandas 1.5.3 which is incompatible.\nautoai-libs 1.14.10 requires scikit-learn<1.2,>=0.20.3; python_version >= \"3.9\", but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scikit-learn-1.2.2\nCollecting sentence-transformers==2.2.2\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.66.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.12.1)\nRequirement already satisfied: torchvision in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.13.1)\nRequirement already satisfied: numpy in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.24.2)\nRequirement already satisfied: scikit-learn in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.8.1)\nCollecting nltk\n  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: sentencepiece in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.1.96)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.16.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.7.1)\nRequirement already satisfied: filelock in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.12.3)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0)\nRequirement already satisfied: requests in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\nRequirement already satisfied: fsspec in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2021.10.1)\nCollecting regex!=2019.12.17\n  Downloading regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m771.9/771.9 kB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting safetensors>=0.3.1\n  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: click in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from nltk->sentence-transformers==2.2.2) (8.0.4)\nRequirement already satisfied: joblib in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from nltk->sentence-transformers==2.2.2) (1.1.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2) (2.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from torchvision->sentence-transformers==2.2.2) (9.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.0.9)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (1.26.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.7.22)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=602065f7155303a15cc3cc70e3b1c53f345855886457cc5c9aee4f535b4ccc86\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\nSuccessfully built sentence-transformers\nInstalling collected packages: tokenizers, safetensors, regex, nltk, transformers, sentence-transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.14.0\n    Uninstalling tokenizers-0.14.0:\n      Successfully uninstalled tokenizers-0.14.0\nSuccessfully installed nltk-3.8.1 regex-2023.8.8 safetensors-0.3.3 sentence-transformers-2.2.2 tokenizers-0.13.3 transformers-4.33.1\nCollecting streamlit==1.24.1\n  Downloading streamlit-1.24.1-py2.py3-none-any.whl (8.9 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: blinker<2,>=1.0.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from streamlit==1.24.1) (1.4)\nCollecting toml<2\n  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\nCollecting pympler<2,>=0.9\n  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting protobuf<5,>=3.20\n  Downloading protobuf-4.24.3-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from streamlit==1.24.1) (2.8.2)\nRequirement already satisfied: pillow<10,>=6.2.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from streamlit==1.24.1) (9.3.0)\n", "name": "stdout"}, {"output_type": "stream", "text": "Collecting tzlocal<5,>=1.1\n  Downloading tzlocal-4.3.1-py3-none-any.whl (20 kB)\nCollecting rich<14,>=10.11.0\n  Downloading rich-13.5.2-py3-none-any.whl (239 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m239.7/239.7 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions<5,>=4.0.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from streamlit==1.24.1) (4.7.1)\nCollecting validators<1,>=0.2\n  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\nRequirement already satisfied: tenacity<9,>=8.0.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from streamlit==1.24.1) (8.2.3)\nCollecting altair<6,>=4.0\n  Downloading altair-5.1.1-py3-none-any.whl (520 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m520.6/520.6 kB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyarrow>=4.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from streamlit==1.24.1) (8.0.0)\nRequirement already satisfied: cachetools<6,>=4.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from streamlit==1.24.1) (4.2.2)\nRequirement already satisfied: pandas<3,>=0.25 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from streamlit==1.24.1) (1.5.3)\nRequirement already satisfied: click<9,>=7.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from streamlit==1.24.1) (8.0.4)\nCollecting pydeck<1,>=0.1.dev5\n  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: requests<3,>=2.4 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from streamlit==1.24.1) (2.31.0)\nCollecting gitpython!=3.1.19,<4,>=3\n  Downloading GitPython-3.1.35-py3-none-any.whl (188 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m188.8/188.8 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting watchdog\n  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from streamlit==1.24.1) (1.24.2)\nRequirement already satisfied: packaging<24,>=14.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from streamlit==1.24.1) (21.3)\nRequirement already satisfied: importlib-metadata<7,>=1.4 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from streamlit==1.24.1) (4.11.3)\nRequirement already satisfied: tornado<7,>=6.0.3 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from streamlit==1.24.1) (6.3.2)\nRequirement already satisfied: toolz in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit==1.24.1) (0.11.2)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit==1.24.1) (4.4.0)\nRequirement already satisfied: jinja2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit==1.24.1) (3.0.3)\nCollecting gitdb<5,>=4.0.1\n  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from importlib-metadata<7,>=1.4->streamlit==1.24.1) (3.8.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from packaging<24,>=14.1->streamlit==1.24.1) (3.0.9)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from pandas<3,>=0.25->streamlit==1.24.1) (2022.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from python-dateutil<3,>=2->streamlit==1.24.1) (1.16.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests<3,>=2.4->streamlit==1.24.1) (1.26.11)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests<3,>=2.4->streamlit==1.24.1) (2023.7.22)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests<3,>=2.4->streamlit==1.24.1) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from requests<3,>=2.4->streamlit==1.24.1) (3.3)\nCollecting markdown-it-py>=2.2.0\n  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pygments<3.0.0,>=2.13.0\n  Downloading Pygments-2.16.1-py3-none-any.whl (1.2 MB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pytz-deprecation-shim\n  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\nCollecting smmap<6,>=3.0.1\n  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from jinja2->altair<6,>=4.0->streamlit==1.24.1) (2.1.1)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.24.1) (0.18.0)\nRequirement already satisfied: attrs>=17.4.0 in /opt/conda/envs/Python-3.10/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.24.1) (21.4.0)\nCollecting mdurl~=0.1\n  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nCollecting tzdata\n  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: watchdog, validators, tzdata, toml, smmap, pympler, pygments, protobuf, mdurl, pytz-deprecation-shim, pydeck, markdown-it-py, gitdb, tzlocal, rich, gitpython, altair, streamlit\n  Attempting uninstall: pygments\n    Found existing installation: Pygments 2.11.2\n    Uninstalling Pygments-2.11.2:\n      Successfully uninstalled Pygments-2.11.2\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.19.6\n    Uninstalling protobuf-3.19.6:\n      Successfully uninstalled protobuf-3.19.6\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.9.3 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.24.3 which is incompatible.\ntensorflow-metadata 1.8.0 requires protobuf<4,>=3.13, but you have protobuf 4.24.3 which is incompatible.\ntensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.24.3 which is incompatible.\npytorch-lightning 1.6.5 requires protobuf<=3.20.1, but you have protobuf 4.24.3 which is incompatible.\nonnx 1.12.0 requires protobuf<=3.20.1,>=3.12.2, but you have protobuf 4.24.3 which is incompatible.\nmysql-connector-python 8.0.33 requires protobuf<=3.20.3,>=3.11.0, but you have protobuf 4.24.3 which is incompatible.\nautoai-ts-libs 2.0.13 requires numpy<1.24,>=1.19.2; python_version >= \"3.9\", but you have numpy 1.24.2 which is incompatible.\nautoai-ts-libs 2.0.13 requires scikit-learn<=1.1.1,>=1.0.2; python_version >= \"3.9\", but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed altair-5.1.1 gitdb-4.0.10 gitpython-3.1.35 markdown-it-py-3.0.0 mdurl-0.1.2 protobuf-4.24.3 pydeck-0.8.1b0 pygments-2.16.1 pympler-1.0.1 pytz-deprecation-shim-0.1.0.post0 rich-13.5.2 smmap-5.0.0 streamlit-1.24.1 toml-0.10.2 tzdata-2023.3 tzlocal-4.3.1 validators-0.22.0 watchdog-3.0.0\n", "name": "stdout"}]}, {"metadata": {}, "id": "26f46269", "cell_type": "code", "source": "import os\nimport requests\nimport chromadb\nimport pandas as pd\nfrom typing import Optional, Any, Iterable, List\n\nfrom dotenv import load_dotenv\n\ntry:\n    from sentence_transformers import SentenceTransformer\nexcept ImportError:\n    raise ImportError(\"Could not import sentence_transformers: Please install sentence-transformers package.\")\n    \ntry:\n    import chromadb\n    from chromadb.api.types import EmbeddingFunction\nexcept ImportError:\n    raise ImportError(\"Could not import chromdb: Please install chromadb package.\")\n    \nfrom typing import Dict, Optional, List\nfrom rouge import Rouge\n\n\nfrom ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams", "execution_count": 2, "outputs": []}, {"metadata": {}, "id": "8dfea2fa", "cell_type": "markdown", "source": "#### 1.3. Load credentials for `ibm-watson-machine-learning`\n\n\n```\nAPI_KEY=<your-api_key>\nIBM_CLOUD_URL=<your-url>\nPROJECT_ID=<your-project_id>\n```"}, {"metadata": {}, "id": "9f5a9bef", "cell_type": "code", "source": "from getpass import getpass\nIBM_CLOUD_API_KEY = getpass(\"Enter your IBM CLoud API Key: \")\nIBM_CLOUD_URL= 'https://us-south.ml.cloud.ibm.com'\nPROJECT_ID = os.getenv(\"PROJECT_ID\")\ncreds = {\n    \"url\": IBM_CLOUD_URL,\n    \"apikey\": IBM_CLOUD_API_KEY\n}\nproject_id = PROJECT_ID", "execution_count": 3, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Enter your IBM CLoud API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"}]}, {"metadata": {}, "id": "1f14b93b", "cell_type": "markdown", "source": "## 2. Index knowledge base"}, {"metadata": {}, "id": "80a5760c", "cell_type": "markdown", "source": "### 2.1. Load data"}, {"metadata": {}, "id": "a0d4d118", "cell_type": "markdown", "source": "Select one of the two dataset available:\n1. *nq910* - an Information Retrieval (a.k.a. search) data set extracted from Google's Natural Questions dataset.\n2. *LongNQ* - an end-to-end retrieval and answer dataset extracted from the same NQ dataset, but focused more on abstractive question answering.\n\nThese datasets are provided under the /data directory."}, {"metadata": {}, "id": "6c5d904a", "cell_type": "code", "source": "datasets = ['LongNQ', 'nq910']\ndataset = datasets[0]    # The current dataset to use\ndata_root = \"data\"\ndata_dir = os.path.join(data_root, dataset)\nmax_docs = -1\nprint(\"Selected dataset:\", dataset)", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "Selected dataset: LongNQ\n", "name": "stdout"}]}, {"metadata": {}, "id": "4109b1c1", "cell_type": "code", "source": "import os, types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\ncos_client = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='g5DSKZVs5CucX2Rq4RJI-5vP9VYpcHpyl15bNbT90Nmo',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.private.us-south.cloud-object-storage.appdomain.cloud')\n\nbucket = 'watsonxaifundamentals-donotdelete-pr-lsrsglkyrlpqqd'\nobject_key = 'questions.csv'\n\nbody = cos_client.get_object(Bucket=bucket,Key=object_key)['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\nquestions = pd.read_csv(body)\nquestions.head()", "execution_count": 5, "outputs": [{"output_type": "execute_result", "execution_count": 5, "data": {"text/plain": "    qid                                               text relevant  \\\n0  7707        who was president when idaho became a state        9   \n1  8127  when did dolly parton release i will always lo...       25   \n2  3545         pro kabaddi 2017 how many matches per team      104   \n3  3745  when was the first airline meal served during ...      121   \n4  8519  when was the cathedral of santa maria del fior...      136   \n\n                            answers  \n0                 Benjamin Harrison  \n1                           in 1974  \n2                                22  \n3                              1919  \n4  begun in 1296::completed by 1436  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>text</th>\n      <th>relevant</th>\n      <th>answers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7707</td>\n      <td>who was president when idaho became a state</td>\n      <td>9</td>\n      <td>Benjamin Harrison</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8127</td>\n      <td>when did dolly parton release i will always lo...</td>\n      <td>25</td>\n      <td>in 1974</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3545</td>\n      <td>pro kabaddi 2017 how many matches per team</td>\n      <td>104</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3745</td>\n      <td>when was the first airline meal served during ...</td>\n      <td>121</td>\n      <td>1919</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8519</td>\n      <td>when was the cathedral of santa maria del fior...</td>\n      <td>136</td>\n      <td>begun in 1296::completed by 1436</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "id": "41330644", "cell_type": "code", "source": "\nimport os, types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\ncos_client = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='g5DSKZVs5CucX2Rq4RJI-5vP9VYpcHpyl15bNbT90Nmo',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.private.us-south.cloud-object-storage.appdomain.cloud')\n\nbucket = 'watsonxaifundamentals-donotdelete-pr-lsrsglkyrlpqqd'\nobject_key = 'output.csv'\n\nbody = cos_client.get_object(Bucket=bucket,Key=object_key)['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndocuments = pd.read_csv(body)\ndocuments.head()\n\n", "execution_count": 6, "outputs": [{"output_type": "execute_result", "execution_count": 6, "data": {"text/plain": "    id                                               text             title\n0    1  History of Idaho - wikipedia History of Idaho ...  History of Idaho\n1    2  1957 . Location Cataldo , Idaho Built 1848 Arc...  History of Idaho\n2    3  of the Columbia was created in June 1816 , and...  History of Idaho\n3    4  Canyon , he concluded that water transport was...  History of Idaho\n4    5  1842 , Father Pierre - Jean De Smet , with Fr....  History of Idaho", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>History of Idaho - wikipedia History of Idaho ...</td>\n      <td>History of Idaho</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1957 . Location Cataldo , Idaho Built 1848 Arc...</td>\n      <td>History of Idaho</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>of the Columbia was created in June 1816 , and...</td>\n      <td>History of Idaho</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Canyon , he concluded that water transport was...</td>\n      <td>History of Idaho</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1842 , Father Pierre - Jean De Smet , with Fr....</td>\n      <td>History of Idaho</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "id": "ee2fe3c4", "cell_type": "code", "source": "documents[\"id\"] = documents.rename({' id':'id'}, inplace=True)", "execution_count": 7, "outputs": []}, {"metadata": {}, "id": "ea5d87ce", "cell_type": "markdown", "source": "Selecting only 2000 examples to optimize time while creating embeddings. If we want to utilize the entire file for the pre-processing, comment the following and only use the entire dataframe."}, {"metadata": {}, "id": "4d9fde6e", "cell_type": "code", "source": "#If you want to read from local: Please uncomment these lines\n#passages = pd.read_csv(os.path.join(data_dir, \"passages.tsv\"), sep='\\t', header=0)\n#qas = pd.read_csv(os.path.join(data_dir, \"questions.tsv\"), sep='\\t', header=0).rename(columns={\"text\": \"question\"})\n\n# We only use 2000 examples.  Comment the lines below to use the full dataset.\ndocuments = documents.head(2000)\nquestions = questions.head(2000)\n\n#documents, questions = load_data_v1(data_dir, data_root)\ndocuments['indextext'] = documents['title'].astype(str) + \"\\n\" + documents['text']", "execution_count": 8, "outputs": []}, {"metadata": {}, "id": "8b06d45f", "cell_type": "markdown", "source": "#### 1.2. Create an embedding function\n\nNote that you can feed a custom embedding function to be used by chromadb. The performance of chromadb may differ depending on the embedding model used."}, {"metadata": {}, "id": "697431ff", "cell_type": "code", "source": "class MiniLML6V2EmbeddingFunction(EmbeddingFunction):\n    MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n    def __call__(self, texts):\n        return MiniLML6V2EmbeddingFunction.MODEL.encode(texts).tolist()\nemb_func = MiniLML6V2EmbeddingFunction()", "execution_count": 9, "outputs": [{"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)e9125/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "89ba75c0ee5148e584d39cf57b3fa0f7"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "66b95843779b47e69b3d75e779132465"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)7e55de9125/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "4a5042291d72418d98e0653572a0c0fb"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "3e6d0162d7b843f9b74bd9ec5a3ea984"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c23ed570a8514fc3a2093c3eb8af21cc"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)125/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9261f4c5417545faa7eea3175cc7c706"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0e489cfd19544164984cdfd0aef97fee"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e601056a67364aab8c64c498d3f495d4"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "a045df700a7d46e3b16b92d648280521"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)e9125/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "ed6372ad05c3431dbfa7a6880f1f05e7"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8392a9ad55384ecd9a91c0066d4f0225"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)9125/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2465b26f354a4a3da008b42d940ac455"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)7e55de9125/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "7a0977fd3e444b5e8b3b29711cb6b23c"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f2d4b38b526344efbb7d9ba3eb1d217e"}}, "metadata": {}}]}, {"metadata": {}, "id": "757aff54", "cell_type": "markdown", "source": "#### 2.3. Set up Chroma upsert\nUpserting a document means update the document even if it exists in the database. Otherwise re-inserting a document throws an error. This is useful for experimentation purpose."}, {"metadata": {}, "id": "be82bd88", "cell_type": "code", "source": "class ChromaWithUpsert:\n    def __init__(self, name,persist_directory, embedding_function,collection_metadata: Optional[Dict] = None,\n    ):\n        self._client = chromadb.PersistentClient(path=persist_directory)\n        self._embedding_function = embedding_function\n        self._persist_directory = persist_directory\n        self._name = name\n        self._collection = self._client.get_or_create_collection(\n            name=self._name,\n            embedding_function=self._embedding_function\n            if self._embedding_function is not None\n            else None,\n            metadata=collection_metadata,\n        )\n\n    def upsert_texts(\n        self,\n        texts: Iterable[str],\n        metadata: Optional[List[dict]] = None,\n        ids: Optional[List[str]] = None,\n        **kwargs: Any,\n    ) -> List[str]:\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\n        Args:\n            :param texts (Iterable[str]): Texts to add to the vectorstore.\n            :param metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n            :param ids (Optional[List[str]], optional): Optional list of IDs.\n            :param metadata: Optional[List[dict]] - optional metadata (such as title, etc.)\n        Returns:\n            List[str]: List of IDs of the added texts.\n        \"\"\"\n        # TODO: Handle the case where the user doesn't provide ids on the Collection\n        if ids is None:\n            import uuid\n            ids = [str(uuid.uuid1()) for _ in texts]\n        embeddings = None\n        self._collection.upsert(\n            metadatas=metadata, documents=texts, ids=ids\n        )\n        return ids\n\n    def is_empty(self):\n        return self._collection.count()==0\n\n    def query(self, query_texts:str, n_results:int=5):\n        \"\"\"\n        Returns the closests vector to the question vector\n        :param query_texts: the question\n        :param n_results: number of results to generate\n        :return: the closest result to the given question\n        \"\"\"\n        return self._collection.query(query_texts=query_texts, n_results=n_results)", "execution_count": 10, "outputs": []}, {"metadata": {}, "id": "a934ce63", "cell_type": "markdown", "source": "#### 2.4 Embed and index documents with Chroma\nYou will now generate embeddings for the passages. This will take\n\nHowever if you want to full experience, then delete these files and rebuild them yourself. Note that creating the embeddings and indexes can take a long time. E.g. on a 2021 Macbook Pro, it took 45 mins to generate these files for the LongNQ dataset."}, {"metadata": {}, "id": "7ad9b3b8", "cell_type": "code", "source": "%%time\nchroma = ChromaWithUpsert(\n    name=f\"{dataset}_minilm6v2\",\n    embedding_function=emb_func,  # you can have something here using /embed endpoint\n    persist_directory=data_dir,\n)\nif chroma.is_empty():\n    _ = chroma.upsert_texts(\n        texts=documents.indextext.tolist(),\n        # we handle tokenization, embedding, and indexing automatically. You can skip that and add your own embeddings as well\n        metadata=[{'title': title, 'id': id}\n                  for (title,id) in\n                  zip(documents.title, documents[' id'])],  # filter on these!\n        ids=[str(i) for i in documents[' id']],  # unique for each doc\n    )", "execution_count": 11, "outputs": [{"output_type": "stream", "text": "CPU times: user 4min 17s, sys: 2min 40s, total: 6min 58s\nWall time: 3min 42s\n", "name": "stdout"}]}, {"metadata": {}, "id": "b286d93a", "cell_type": "markdown", "source": "### 3. Generate a retrieval-augmented response to a question\n3.1. Instantiate watsonx model"}, {"metadata": {}, "id": "5dd7eaa2", "cell_type": "code", "source": "params = {\n        GenParams.DECODING_METHOD: \"greedy\",\n        GenParams.MIN_NEW_TOKENS: 1,\n        GenParams.MAX_NEW_TOKENS: 100,\n        GenParams.TEMPERATURE: 0,\n    }\nmodel = Model(model_id='google/flan-ul2', params=params, credentials=creds, project_id=project_id)", "execution_count": 12, "outputs": []}, {"metadata": {}, "id": "234e4058", "cell_type": "markdown", "source": "#### 3.2. Select a question"}, {"metadata": {}, "id": "9d9c6323", "cell_type": "code", "source": "question_index = 23\nquestion_text = questions.text[question_index].strip(\"?\") + \"?\"\nprint(question_text)", "execution_count": 13, "outputs": [{"output_type": "stream", "text": "who was the old woman in phantom of the opera?\n", "name": "stdout"}]}, {"metadata": {}, "id": "ef7bd857", "cell_type": "markdown", "source": "#### 3.3. Retrieve relevant context\n"}, {"metadata": {}, "id": "040ac1bf", "cell_type": "code", "source": "relevant_chunks = chroma.query(\n    query_texts=[question_text],\n    n_results=5,\n)\nfor i, chunk in enumerate(relevant_chunks['documents'][0]):\n    print(\"=========\")\n    print(\"Paragraph index : \", relevant_chunks['ids'][0][i])\n    print(\"Paragraph : \", chunk)\n    print(\"Distance : \", relevant_chunks['distances'][0][i])", "execution_count": 14, "outputs": [{"output_type": "stream", "text": "=========\nParagraph index :  644\nParagraph :  Madame Giry\na meeting with the managers , she explains that she once found a note in Box Five , written by the Ghost and listing a number of dancers and singers who married into royalty and the nobility , with her daughter Meg included as eventually becoming an empress . This note was enough to persuade Madame Giry to help the Ghost ; later in life , Meg did become the Baroness de Castelot - Barbezac . Madame Giry in the 1986 the Phantom of the Opera musical ( edit ) In Andrew Lloyd Webber 's The Phantom of the Opera , Madame Giry 's role is changed to become a slightly younger woman who now works as a choreographer of the corps de ballet . She is shown holding a cane , which she uses to beat time with the music , but which she is rarely shown to use otherwise . When the Opera Diva , Carlotta Giudicelli , walks out during rehearsals at the start of the show , it is Madame Giry , and her daughter , Meg , who suggest Christine Daa\u00e9 for the leading role . Later in the show , she receives one of the Phantom 's `` Notes '' in both reprises of this tune . The first one she gives to M. Firmin , but the second she reads herself , being over-spoken by the Phantom part way through . While there is very little history given between the Phantom and Madame\nDistance :  0.7307764291763306\n=========\nParagraph index :  646\nParagraph :  Madame Giry\nMadame Giry and the Persian from the novel . ( In the novel , the Persian shows Raoul where the Opera Ghost resides - although he does accompany Raoul , unlike Madame Giry . ) They both tell Raoul to hold his hand `` at the level of his eyes '' , to protect from attack with punjab lasso . Madame Giry is not amongst the crowd who enter the Phantom 's Lair at the end of the musical , although her daughter Meg Giry is . In the film version , Richardson is seen playing a non-speaking character in several of the black - and - white scenes , which indicate events many years after the events in the story . Since Raoul uses a wheelchair in these scenes , it is wrongly assumed that she is meant to be an elder Meg , as opposed to Madame Giry . References ( edit ) The Phantom of the Opera by Gaston Leroux Books The Phantom of the Opera ( 1909 -- 1910 ) Phantom ( 1990 ) The Canary Trainer ( 1993 ) The Phantom of Manhattan ( 1999 ) Stage adaptations Phantom of the Opera ( 1976 ) The Phantom of the Opera ( 1986 ) Phantom ( 1991 ) Love Never Dies ( 2010 ) Film and television Das Phantom der Oper ( 1916 ) The Phantom of the Opera ( 1925 ) Song at Midnight ( 1937 ) Phantom of the Opera ( 1943 ) The\nDistance :  0.7543473839759827\n=========\nParagraph index :  642\nParagraph :  Madame Giry\nMadame Giry - wikipedia Madame Giry This article needs additional citations for verification . Please help improve this article by adding citations to reliable sources . Unsourced material may be challenged and removed . ( February 2009 ) ( Learn how and when to remove this template message ) Madame Giry The Phantom of the Opera character Giry as portrayed by Miranda Richardson in the 2004 film version . First appearance The Phantom of the Opera Created by Gaston Leroux Portrayed by Miranda Richardson Information Gender Female Occupation Concierge Children Meg Giry ( daughter ) Nationality French Madame Giry is a fictional character from Gaston Leroux 's 1909 novel The Phantom of the Opera . She is a fairly intermediate character in the novel , although her role is much increased in the Andrew Lloyd Webber musical . This article will deal with both versions separately . Madame Giry is also a character in the musical Love Never Dies , a sequel to The Phantom of the Opera . Madame Giry in the Phantom of the Opera novel ( edit ) Madame Giry is an aging woman who works as a concierge in the rue de Provence , who looks after patrons of the Opera , principally those who sit in the boxes . One night , whilst working , Madame Giry hears a male voice in Box Five , which she knows to be empty . After recovering from her surprise , she learns to trust the `` voice\nDistance :  0.8183770775794983\n=========\nParagraph index :  645\nParagraph :  Madame Giry\nGiry in the musical , a small scene was extended in the film version , showing a young Madame Giry rescuing the Phantom from a traveling circus and hiding him in the Opera House . This was taken from Frederick Forsyth 's sequel , The Phantom of Manhattan . Even in the extended scene she does not give any indication why she is working for the Phantom ( other than sympathy and her appreciation for his genius ) , unlike the reasons given in the novel . In the 2004 film version of the musical ( in which she is played by Miranda Richardson ) , Madame Giry is shown almost to be an accomplice of the Phantom , failing to notify people when he locks Christine 's dressing room ( though she could have alerted someone and they did n't listen ) or when she spots him in the rafters during a scene . This collusion does not appear in the stage musical . Regularly during the musical , Madame Giry tries to stop the Managers from doing any harm to the Phantom and defends him on several occasions . However , when Christine is abducted by the Phantom , Madame Giry gives up all hope of avoiding her past and helps Raoul de Chagny to the Phantom 's Lair deep underground . She shows him the way , but will not accompany him into the catacombs . In this way , Andrew Lloyd Webber combines the roles of\nDistance :  0.8252385854721069\n=========\nParagraph index :  647\nParagraph :  Madame Giry\nPhantom of the Opera ( 1962 ) The Phantom of Hollywood ( 1974 ) Phantom of the Paradise ( 1974 ) Opera ( 1987 ) The Phantom of the Opera ( 1989 ) Phantom of the Mall : Eric 's Revenge ( 1989 ) The Phantom of the Opera ( 1990 ) The Phantom Lover ( 1995 ) The Phantom of the Opera ( 1998 ) Phantom of the Megaplex ( 2000 ) The Phantom of the Opera ( 2004 ) The Phantom of the Opera at the Royal Albert Hall ( 2011 ) Characters Erik Christine Daa\u00e9 Viscount Raoul de Chagny The Persian Carlotta Madame Giry Meg Giry Joseph Buquet Songs `` The Phantom of the Opera '' `` The Music of the Night '' `` All I Ask of You '' `` Learn to Be Lonely '' `` ' Til I Hear You Sing '' Other Don Juan Triumphant Punjab lasso Adaptations Return of the Phantom Palais Garnier Retrieved from `` https://en.wikipedia.org/w/index.php?title=Madame_Giry&oldid=828891370 '' Categories : Characters in The Phantom of the Opera Fictional French people Fictional characters introduced in 1909 Female characters in literature Hidden categories : Articles needing additional references from February 2009 All articles needing additional references Pages using infobox character with unknown parameters Talk About Wikipedia Italiano Edit links This page was last edited on 5 March 2018 , at 11 : 07 ( UTC ) . About Wikipedia\nDistance :  0.8439304232597351\n", "name": "stdout"}]}, {"metadata": {}, "id": "188dc761", "cell_type": "markdown", "source": "#### 3.4. Feed the context and the question to `watsonx` model."}, {"metadata": {}, "id": "cf42f1a9", "cell_type": "code", "source": "def make_prompt(context, question_text):\n    return (f\"{context}\\n\\nPlease answer a question using this \"\n          + f\"text. \"\n          + f\"If the question is unanswerable, say \\\"unanswerable\\\".\"\n          + f\"Question: {question_text}\")", "execution_count": 15, "outputs": []}, {"metadata": {}, "id": "4d3207cd", "cell_type": "code", "source": "context = \"\\n\\n\\n\".join(relevant_chunks[\"documents\"][0])\nprompt = make_prompt(context, question_text)", "execution_count": 16, "outputs": []}, {"metadata": {}, "id": "39cfea31", "cell_type": "code", "source": "response = model.generate_text(prompt)\n", "execution_count": 17, "outputs": []}, {"metadata": {}, "id": "500b3614", "cell_type": "code", "source": "print(\"Question = \", question_text)\nprint(\"Answer = \", response)\nprint(\"Expected Answer(s) (may not be appear with exact wording in the dataset) = \", questions.answers[question_index])", "execution_count": 18, "outputs": [{"output_type": "stream", "text": "Question =  who was the old woman in phantom of the opera?\nAnswer =  Madame Giry\nExpected Answer(s) (may not be appear with exact wording in the dataset) =  Madame Giry\n", "name": "stdout"}]}, {"metadata": {}, "id": "573c1220", "cell_type": "markdown", "source": "### 4. Evaluate RAG performance on your data\nEvaluating the performance of your Generative AI system is critical to ensuring happy end users. However evaluation also requires having a test dataset. In this case, the top passages that shoudl be return for each question.\n\nNote that we want to evaluate the performance of both (1) the embedding function plus (2) how well the GenAI model summarizes the results.\n\nSo our test set must contain:\n\nThe indexes of the passage(s) that contain the answer - i.e. the goldstandard passages (if the question is answerable by the knowledge base)\nThe question's gold standard answer (this can be short or long-form)\n\n\n4.1. Evaluate the retrieval quality\nWere the correct passages returned via the similarity search functionality\n\nThere are many ways to compute retrieval quality, namely how the information contained in the documents that are relevant to the question being asked. We're focusing here on success at given number of returns (aka recall at given levels), which is to say, given a fixed number of documents returned (e.g., 1, 3, 5), is the question's answer contained in them. The scores increase with the recall level."}, {"metadata": {}, "id": "49901af8", "cell_type": "code", "source": "def compute_score(questions, answers, ranks=[1, 3, 5, 10], use_rouge=False, rouge_threshold=0.7):\n    \"\"\"\n    Computes the success at different levels of recall, given the goldstandard passage indexes per query.\n    It computes two scores:\n       * Success at rank_i, defined as sum_q 1_{top i answers for question q contains a goldstandard passage} / #questions\n       * Lenient success at rank i, defined as\n                sum_q 1_{ one in the documents in top i for question q contains a goldstandard answer) / #questions\n    Note that a document that contains the actual textual answer does not necesarily answer the question, hence it's a\n    more lenient evaluation. Any goldstandard passage will contain a goldstandard answer text, by definition.\n    Args:\n        :param questions: List[Dict['id': AnyStr, 'text': AnyStr, 'relevant': AnyStr, 'answers': AnyStr]]\n           - the input queries. Each query is a dictionary with the keys 'id','text', 'relevant', 'answers'.\n        :param input_passages: List[Dict['id': AnyStr, 'text': AnyStr', 'title': AnyStr]]\n           - the input passages. These are used to create a reverse-index list for the passages (so we can get the\n             text for a given passage ID)\n        :param answers: List[List[AnyStr]]\n           - the retrieved passages IDs for each query\n        :param ranks: List[int]\n           - the ranks at which to compute success\n        :param use_rouge: Boolean\n           - turns on the use of rouge as a scorer\n        :param rouge_threshold: float, default=0.7\n           - defines the minimum rouge-l/r score to accept the answer as a match,\n    Returns:\n\n\n    \"\"\"\n    # if \"relevant\" not in input_queries[0] or input_queries[0]['relevant'] is None:\n    #     print(\"The input question file does not contain answers. Please fix that and restart.\")\n    #     sys.exit(12)\n\n    scores = {r: 0 for r in ranks}\n    lscores = {r: 0 for r in ranks}\n\n    gt = {}\n    for q_relevant, q_qid in zip(questions.relevant, questions.qid):\n        if isinstance(q_relevant, str):\n            rel = [int(i) for i in q_relevant.split(\",\")]\n        else:\n            rel = [q_relevant]\n        gt[q_qid] = rel\n\n    def update_scores(ranks, rnk, scores):\n        j = 0\n        while j < len(ranks) and ranks[j] < rnk:\n            j += 1\n        for k in ranks[j:]:\n            scores[k] += 1\n\n    scorer = None\n    if use_rouge:\n        from rouge import Rouge\n        scorer = Rouge()\n\n    num_eval_questions = 0\n\n    for qi, (qid, q_answers) in enumerate(zip(questions.qid, questions.answers)):\n        tmp_scores = {r: 0 for r in ranks}\n\n        text_answers = str(q_answers).split(\"::\")\n        if \"-\" in text_answers:\n            # The question does not have answers, skip it for retrieval score purposes.\n            continue\n        num_eval_questions += 1\n        # Compute scores based on the goldstandard annotation\n        for ai, ans in enumerate(answers[qi]):\n            if int(ans['id']) in gt[qid]:  # Great, we found a match.\n                update_scores(ranks, ai + 1, tmp_scores)\n                break\n\n        # Compute score on approximate match - either answer inclusion in the text or\n        # minimum rouge score alignment.\n        tmp_lscores = tmp_scores.copy()  # making sure we're actually lenient\n        #inputq = questions[qi]\n        for ai, ans in enumerate(answers[qi]):\n            txt = ans['text'].lower()\n            found = False\n            for text_answer in text_answers:\n                if use_rouge:\n                    score = scorer.get_scores(text_answer.lower(), txt)\n                    if max(score[0]['rouge-l']['r'], score[0]['rouge-l']['p']) > rouge_threshold:\n                        update_scores(ranks, ai + 1, tmp_lscores)\n                        break\n                else:\n                    if not isinstance(text_answer, str):\n                        print(f\"Error on text_answer {text_answer}, question {qi}, answer {ai}-{ans}\")\n                    if txt.find(text_answer.lower()) >= 1:\n                        update_scores(ranks, ai + 1, tmp_lscores)\n                        break\n\n        for r in ranks:\n            scores[r] += int(tmp_scores[r] >= 1)\n            lscores[r] += int(tmp_lscores[r] >= 1)\n\n    res = {\"num_ranked_queries\": num_eval_questions,\n           \"num_judged_queries\": num_eval_questions,\n           \"success\":\n               {r: int(1000 * scores[r] / num_eval_questions) / 1000.0 for r in ranks},\n           \"lenient_success\":\n               {r: int(1000 * lscores[r] / num_eval_questions) / 1000.0 for r in ranks},\n           \"counts\": {r: scores[r] for r in ranks},\n           'lcounts': {r: lscores[r] for r in ranks}\n           }\n\n    return res", "execution_count": 19, "outputs": []}, {"metadata": {}, "id": "b7180371", "cell_type": "markdown", "source": "#### Compute the retrieval score over all the documents\nCan take up to a minute"}, {"metadata": {}, "id": "12c5b9c9", "cell_type": "code", "source": "k = 5\nretrieved_docs = []\nfor q in questions.text:\n    answers = chroma.query(query_texts=q, n_results=k)\n\n    retrieved_docs.append([{'id': id, 'text': text}\n                           for (id, text) in zip(answers['ids'][0], answers['documents'][0])])\n\nres = compute_score(questions, retrieved_docs,\n                    ranks=[1, 3, 5], use_rouge=(data_dir == 'docs_and_qs'))\nprint(res)", "execution_count": 20, "outputs": [{"output_type": "stream", "text": "{'num_ranked_queries': 910, 'num_judged_queries': 910, 'success': {1: 0.017, 3: 0.049, 5: 0.054}, 'lenient_success': {1: 0.087, 3: 0.165, 5: 0.187}, 'counts': {1: 16, 3: 45, 5: 50}, 'lcounts': {1: 80, 3: 151, 5: 171}}\n", "name": "stdout"}]}, {"metadata": {}, "id": "0950159e", "cell_type": "code", "source": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndef plot(res):\n    fig, ax = plt.subplots()\n    scores = res['success'].values()\n    keys = [f'R@{i}' for i in res['success'].keys()]\n    x_pos = np.arange(len(keys))\n    ax.bar(x_pos, scores, align='center', alpha=0.5)\n    ax.set_ylabel('Success Rate')\n    ax.set_xticks(x_pos)\n    ax.set_xticklabels(keys)\n    ax.set_title('Success rates at different recall rates.')\n    ax.yaxis.grid(True)\n\n    # Save the figure and show\n    plt.tight_layout()\n    plt.savefig('bar_plot.png')\n    plt.show()", "execution_count": 21, "outputs": []}, {"metadata": {}, "id": "0d1f4e83", "cell_type": "code", "source": "plot(res)", "execution_count": 22, "outputs": [{"output_type": "display_data", "data": {"text/plain": "<Figure size 640x480 with 1 Axes>", "image/png": "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\n"}, "metadata": {}}]}, {"metadata": {}, "id": "a9dfafb1", "cell_type": "markdown", "source": "### 4.2. Evaluate quality of generated responses\nI.e. how well did the GenAI model summarize and extract the correct answer to the user's question from the passages returned by the similarity function.  Obviously if the returned passages were invalid, then performance at this phase would suffer too."}, {"metadata": {}, "id": "39e72e5f", "cell_type": "markdown", "source": "##### Automatically evaluating the quality of answers is difficult, as many factors come into play, such as fluency, helpfulness, coverage, etc. One simplified way of computing this quality is using the ROUGE metric, in particular ROUGE-L. To compute this metric, for every answer returned for a question, we measure the maximum subsequence of words between the system answer and the gold-standard answer. Given this sequence, we can compute the precision of the given answer as the length (all lengths are in words) of this sequence divided by the length of the system answer and the recall as the length of the longest common subsequence divided by the length the gold-standard answer.\n$$ P_{ROUGE-L} = \\frac{|lcs(system,gold)|}{|system|} \\\\ R_{ROUGE_L} = \\frac{|lcs(system,gold|}{|gold|} $$\n\nwhere $lcs(system, gold)$ is the longest commong subsequence between $system$ and $gold$.\n\nROUGE was devised in the NLP community to evaluate summarization, and is commonly used to also evaluate abstractive question answering."}, {"metadata": {}, "id": "f23b22b6", "cell_type": "code", "source": "def score_answers(_answers, _reference, score_type=\"rouge-l\", val=\"r\", use_rouge=True):\n    \"\"\"\n    Compute the score of a set of answers, given a set of references, using Rouge score.\n    :param answers: Union[List[str], str]\n       - the returned answer/answers.\n    :param reference:\n        - the reference answers, in a string. Answers are separated by ':::'\n    :param use_rouge: Boolean\n        - if true, then use rouge for scoring, otherwise use substring.\n    :return:\n       - The maximum rouge-L score of the cartesian product of answers/references\n    \"\"\"\n    if isinstance(_answers, str):\n        _answers = [_answers]\n    _references = _reference.lower().split(\"::\")\n    max_score = -1\n    scorer = Rouge()\n    closest_ref = \"\"\n    for ref in _references:\n        for _answer in _answers:\n            if use_rouge:\n                scores = scorer.get_scores(_answer.lower(), ref)\n                score = scores[0][score_type][val]\n            else:\n                score = int(ref.find(_answer.lower()) >= 0)\n            if score > max_score:\n                max_score = score\n                closest_ref = ref\n\n    return max_score, closest_ref", "execution_count": 23, "outputs": []}, {"metadata": {}, "id": "8b48cb08", "cell_type": "code", "source": "print(\"Question = \", question_text)\nprint(\"Answer = \", response)\nscore, closest_ref = score_answers(response, questions.answers[question_index], val='r')\nprint(f\"Closest reference: \\\"{closest_ref}\\\"\")\nprint(f\"Recall:\\t\\t{100*score:5.2f}%\")\nscore, _ = score_answers(response, questions.answers[question_index], val='p')\nprint(f\"Precision:\\t{100*score:5.2f}%\")\n", "execution_count": 24, "outputs": [{"output_type": "stream", "text": "Question =  who was the old woman in phantom of the opera?\nAnswer =  Madame Giry\nClosest reference: \"madame giry\"\nRecall:\t\t100.00%\nPrecision:\t100.00%\n", "name": "stdout"}]}, {"metadata": {}, "id": "c5f04a79", "cell_type": "markdown", "source": "#### Compute (Rouge-based) precision and recall for the entire collection.\nIt takes about 1-2 seconds per question. For a corpus of ~1000 questions, this take can take up to 30min."}, {"metadata": {}, "id": "c7ca204a", "cell_type": "code", "source": "def is_answerable(relevant):\n    return \"-1\" in relevant", "execution_count": 25, "outputs": []}, {"metadata": {}, "id": "fa5db342", "cell_type": "code", "source": "rscore = 0\npscore = 0\nimport tqdm\nnum_eval_questions = 50\neval_questions = questions[:num_eval_questions]\ncount = {\"11\": 0, \"10\": 0, \"01\": 0, \"00\": 0}\nseq = []\nfor (question_text, answers, relevant) in tqdm.tqdm(zip(eval_questions.text, eval_questions.answers, eval_questions.relevant), total=len(eval_questions)):\n    # ans = qa(question.question)\n    relevant_chunks = chroma.query(\n        query_texts=[question_text],\n        n_results=5,\n    )\n    context = \"\\n\\n\\n\".join(relevant_chunks[\"documents\"][0])\n    prompt = make_prompt(context, question_text)\n    ans = model.generate_text(prompt)\n    q_answerable = is_answerable(relevant)\n    if ans == \"unanswerable\":\n        res = \"10\" if q_answerable else \"00\"\n        count [res] += 1\n        if not q_answerable:\n            rscore += 1\n            pscore += 1\n    else:\n        res = \"11\" if q_answerable else \"10\"\n        count[res] += 1\n        if q_answerable:\n            qrscore, _ = score_answers(ans, answers, val='r')\n            rscore += qrscore\n            qpscore, _ = score_answers(ans, answers, val='p')\n            pscore += qpscore\n    seq.append(res)\n", "execution_count": null, "outputs": [{"output_type": "stream", "text": " 44%|\u2588\u2588\u2588\u2588\u258d     | 22/50 [00:19<00:29,  1.05s/it]", "name": "stderr"}]}, {"metadata": {}, "id": "6b12b258", "cell_type": "code", "source": "from IPython.display import HTML, display\ndef displayHTMLTables(*tables):\n    def htmlTable(table):\n        return '<table border=\"2\"><tr>{}</tr></table>'.format(\n                    '</tr><tr>'.join(\n                        '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in table)\n                )\n\n    display(HTML('<table><tr><td>{}</td></tr></table>'.format(\n                \"</td><td>\".join(htmlTable(table) for table in tables))\n))", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "c7f85bd6", "cell_type": "code", "source": "res = [['', 'Overall', 'Answerable questions'],\n       ['Precision', f\"{100*pscore/len(eval_questions):5.2f}\", f\"{100*(pscore-count['00'])/(count['10']+count['11']):5.2f}\"],\n       ['Recall',    f'{100*pscore/len(eval_questions):5.2f}', f\"{188*(rscore-count['00'])/(count['10']+count['11']):5.2f}\"],\n       ]\ncounts = [['Gold/System', 'No Answer', 'Answered'],\n        ['No Answer', count[\"00\"], count[\"01\"]],\n        ['With Answer', count[\"10\"], count[\"11\"]]]#%% md\n\ndisplayHTMLTables(res, [], [], counts)", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 5}